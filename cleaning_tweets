'''This is the code for cleaning dataset'''
import nltk
import re
import ast
import csv
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pandas as pd

# Downloading stopwords and wordnet from NLTK
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

class TweetCleaner:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.punc_table = str.maketrans("", "", string.punctuation)

    def remove_stop_words(text):
      
    # Tokenize the text into individual words
    words = text.split()

    # Stopwords + other words that don't add value to the analysis
    allnonwords = stopwords.words('english') + ['would', 'could', 'said', 'also', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']
    allnonwords = allnonwords + ['urllink', 'blog', 'date', 'me', 'you', 'she', 'her', 'him', 'they', 'them', 'their', 'your', 'yours', 'our', 'ours']
    allnonwords = allnonwords + ['right', 'look', 'first', 'last', 'never', 'thought', 'next', 'around', 'ever', 'always', 'come', 'to', 'thing', 'things', 'people']
    allnonwords = allnonwords + ['many', 'really', 'thing', 'much', 'stuff', 'there', 'hour', 'point', 'mine', 'yours', 'hers',
                                'his', 'theirs', 'ours', 'issue', 'thing', 'did', 'didnt', 'year', 'maio', 'thing', 'best', 'since', 'month', 'feel']
    allnonwords = allnonwords + ['much', 'something', 'someone', 'even', 'well', 'still', 'little', 'always', 'never', 'ever', 'sure', 'sort']
    allnonwords = allnonwords + ['every', 'anything', 'everything', 'nothing', 'everyone', 'everybody', 'everywhere', 'anyone', 'anybody']
    allnonwords = allnonwords + ['anywhere', 'someone', 'somebody', 'somewhere', 'nowhere', 'thing', 'something', 'nothing', 'everything']
    allnonwords = allnonwords + ['always', 'another', 'though', 'without', 'actually', 'do', 'dont', 'will', 'wont', 'can', 'cant']
    allnonwords = allnonwords + ['get', 'got', 'go', 'going', 'know', 'let', 'like', 'make', 'see', 'want', 'come', 'take', 'think']
    allnonwords = allnonwords + ['back', 'great', 'today', 'year', 'good', 'link', 'night', 'went', 'couple', 'say', 'give', 'need', 'make']
    allnonwords = allnonwords + ['youre', 'youve', 'youll', 'youd', 'hes', 'shes', 'its', 'were', 'theyre', 'thats', 'week', 'made', 'remember',
                                 'might', 'getting', 'better', 'real', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhh', 'news', 'new', 'top',
                                 'u', 'day', 'brureau', 'love', 'u', 'do', 'not', 'well', 'fuck', 'na', 'haha', 'post', 'there',
                                 'anyway', 'ask', 'that', 'mean', 'dunno', 'file', 'miss', 'true', 'point', 'call', 'came', 'look',
                                 'site', 'na', 'talk', 'place', 'need', 'there', 'blog', 'entry', 'originally', 'posted', 'show', 'start',
                                 'okay', 'lots', 'finally', 'yippee', 'comes', 'hello', 'late', 'wish', 'weblog', 'damit', 'dammit',
                                 'currently', 'lala', 'opposite', 'told', 'update', 'updating', 'sometimes', 'maybe',
                                 'easy', 'half', 'different', 'called', 'total', 'took', 'word', 'done', 'stay', 'fine', 'find', 'cannot',
                                 'front', 'back', 'dude', 'feel', 'name', 'time', 'man', 'woman', 'home', 'ching', 'year',
                                 'times', 'yeah', 'sorry', 'whole', 'pretty', 'guess', 'nice', 'tomorrow']
                        


    # Remove stop words from the list of words
    filtered_words = [word for word in words if word not in allnonwords]
  
    # Join the filtered words back into a single string
    filtered_text = ' '.join(filtered_words)
  
    return filtered_text

def remove_short_words(text):
    # Split the text into words
    words = text.split()

    # Remove words with three letters or less
    filtered_words = [word for word in words if len(word) > 3]

    # Join the filtered words back into a text
    filtered_text = ' '.join(filtered_words)

    return filtered_text

def remove_dates(text):
    # Define regex pattern to match months and days of the week
    pattern = r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december|monday|tuesday|wednesday|thursday|friday|saturday|sunday)\b'

    # Remove matched dates using regex substitution
    result = re.sub(pattern, '', text)
    return result

def remove_tags(text):
    # Remove HTML tags using regex
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'<*?>', '', text)
    return text

def remove_numbers(text):
    pattern = r'\d+'  # Regular expression pattern to match numbers
    result = re.sub(pattern, '', text)  # Replace the pattern with empty spaces
    return result

def remove_punctuation_and_newlines(text):
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Remove newline characters
    # text = text.replace('\n', '')
    return text

def lemmatize_text(text):
    # Tokenize the text into individual words
    tokens = word_tokenize(text)

    # Initialize the WordNetLemmatizer
    lemmatizer = WordNetLemmatizer()

    # Lemmatize each token in the text
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Join the lemmatized tokens back into a single string
    lemmatized_text = " ".join(lemmatized_tokens)

    return lemmatized_text

def stem_text(text):
    # Tokenize the text into words
    words = word_tokenize(text)

    # Initialize Porter stemmer
    stemmer = PorterStemmer()

    # Apply stemming to each word
    stemmed_words = [stemmer.stem(word) for word in words]

    # Join the stemmed words back into a text
    stemmed_text = ' '.join(stemmed_words)

    return stemmed_text


def compound_word_split(self, compound_word):
        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', compound_word)
        return [m.group(0) for m in matches]

    def remove_non_ascii_chars(self, text):
        return ''.join([w if ord(w) < 128 else ' ' for w in text])

    def remove_hyperlinks(self,text):
        return ' '.join([w for w in text.split(' ')  if not 'http' in w])

    def get_cleaned_text(self, text):
        cleaned_tweet = text.replace('\"','').replace('\'','').replace('-',' ')
        cleaned_tweet =  self.remove_non_ascii_chars(cleaned_tweet)
        if re.match(r'RT @[_A-Za-z0-9]+:',cleaned_tweet):
            cleaned_tweet = cleaned_tweet[cleaned_tweet.index(':')+2:]
        cleaned_tweet = self.remove_hyperlinks(cleaned_tweet)
        cleaned_tweet = cleaned_tweet.replace('#','HASHTAGSYMBOL').replace('@','ATSYMBOL')
        tokens = [w.translate(self.punc_table) for w in word_tokenize(cleaned_tweet)]
        tokens = [nltk.WordNetLemmatizer().lemmatize(w) for w in tokens if not w.lower() in self.stop_words and len(w)>1]
        cleaned_tweet = ' '.join(tokens)
        cleaned_tweet = cleaned_tweet.replace('HASHTAGSYMBOL','#').replace('ATSYMBOL','@')
        return cleaned_tweet

    def clean_tweets(self, tweets, is_bytes = False):   
        test_tweet_list = []
        for tweet in tweets:
            if is_bytes:
                test_tweet_list.append(self.get_cleaned_text(ast.literal_eval(tweet).decode("UTF-8")))
            else:
                test_tweet_list.append(self.get_cleaned_text(tweet))
        return test_tweet_list
    
    def clean_single_tweet(self, tweet, is_bytes = False):  
        if is_bytes:
             return self.get_cleaned_text(ast.literal_eval(tweet).decode("UTF-8"))
        return self.get_cleaned_text(tweet)
    
    def cleaned_file_creator(self, op_file_name, value1, value2):
        csvFile = open(op_file_name, 'w+')
        csvWriter = csv.writer(csvFile)
        for tweet in range(len(value1)):
            csvWriter.writerow([value1[tweet], value2[tweet]])
        csvFile.close()

# Instantiate the TweetCleaner
cleaner = TweetCleaner()

# Clean the tweets in the DataFrame
tweets['cleaned_body'] = tweets['body'].apply(cleaner.clean_single_tweet)

# Save the cleaned DataFrame to a CSV file
tweets.to_csv('/Users/.../cleaned_merged_tweets.csv', index=False)
print(tweets.columns)

